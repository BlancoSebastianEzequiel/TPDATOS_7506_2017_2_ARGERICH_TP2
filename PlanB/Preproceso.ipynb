{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproceso del set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToUseInitialy = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'place_name',\n",
    "    'property_type',\n",
    "    'surface_total_in_m2',\n",
    "    'surface_covered_in_m2',\n",
    "    'price_aprox_usd',\n",
    "    'price_usd_per_m2',\n",
    "    'rooms',\n",
    "    'place_with_parent_names'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregado de datos de extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# CHANGE STRING COMMA FOR POINT\n",
    "#---------------------------------------------------------\n",
    "def changeStringCommaForPoint(string):\n",
    "    point = \".\"\n",
    "    split = string.split(',')\n",
    "    return point.join(split)\n",
    "#---------------------------------------------------------\n",
    "# CHANGE STRING LIST COMMA FOR POINT\n",
    "#---------------------------------------------------------\n",
    "def changeStringListCommaForPoint(stringList):\n",
    "    aux = []\n",
    "    for string in stringList:\n",
    "        aux.append(changeStringCommaForPoint(string))\n",
    "    return aux\n",
    "#---------------------------------------------------------\n",
    "# MANHATTAN DISTANCE\n",
    "#---------------------------------------------------------\n",
    "def ManhattanDistance(lat1, lon1, lat2, lon2):\n",
    "    # pasamos la diferencia a metros (90° son 10000 Km)\n",
    "    dlat = abs(lat1-lat2) * (10000/90)\n",
    "    # pasamos la diferencia a metros (360° son 40000 Km)\n",
    "    dlon = abs(lon1-lon2) * (40000/360) \n",
    "    distKM = ( (dlat ** 2) + (dlon ** 2) ) ** (0.5)\n",
    "    return float(distKM * 1000)\n",
    "#---------------------------------------------------------\n",
    "# DISTANCE ANALYSIS\n",
    "#---------------------------------------------------------\n",
    "def distanceAnalysis(df, extraDf, lat, lon, distanceName):\n",
    "    # x) lon\n",
    "    # y) lat\n",
    "    extraDf.loc[:, [lon]] = extraDf.loc[:, [lon]].apply(lambda x: float(x), axis = 1)\n",
    "    extraDf.loc[:, [lat]] = extraDf.loc[:, [lat]].apply(lambda x: float(x), axis = 1)\n",
    "    extraDf = extraDf[~np.isnan(extraDf[lon]) | ~np.isnan(extraDf[lat])]\n",
    "\n",
    "    df = df[~np.isnan(df['lon']) | ~np.isnan(df['lat'])]\n",
    "\n",
    "    latDf = df['lat'].tolist()\n",
    "    lonDf = df['lon'].tolist()\n",
    "    x = extraDf[lon].tolist()\n",
    "    y = extraDf[lat].tolist()\n",
    "\n",
    "    distances = []\n",
    "    minor = 0\n",
    "\n",
    "    for i in range(0, len(latDf)):\n",
    "        minor = ManhattanDistance(y[0], x[0], latDf[i], lonDf[i])\n",
    "        for j in range(1, len(x)):\n",
    "            dist = ManhattanDistance(y[j], x[j], latDf[i], lonDf[i])\n",
    "            if (dist < minor):\n",
    "                minor = dist\n",
    "        distances.append(minor)\n",
    "\n",
    "    df[distanceName] = distances\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG SUBWAYS\n",
    "#---------------------------------------------------------\n",
    "def aggSubways(df):\n",
    "    subways = pd.read_csv(\"./extra/estaciones-de-subte.csv\", low_memory = False)\n",
    "    df = distanceAnalysis(df, subways, 'Y', 'X', 'distanceSubway')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG UNIVERSITIES\n",
    "#---------------------------------------------------------\n",
    "def aggUniversities(df):\n",
    "    universities = pd.read_csv(\"./extra/universidades.csv\", low_memory = False, sep=';')\n",
    "    df = distanceAnalysis(df, universities, 'LAT', 'LNG', 'distanceUniversities')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG HOSPITALES\n",
    "#---------------------------------------------------------\n",
    "def aggHospitales(df):\n",
    "    hospitales = pd.read_csv(\"./extra/hospitales.csv\", low_memory = False, sep=';')\n",
    "    df = distanceAnalysis(df, hospitales, 'LAT', 'LNG', 'distanceHospitales')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# GET DATAFRAME COORDS BY PLACES\n",
    "#---------------------------------------------------------\n",
    "def getDataframeCoordsByPlaces(df):\n",
    "    aux = df\n",
    "    aux = aux.loc[:, ['lat', 'lon', 'place_name']]\\\n",
    "    .groupby('place_name').agg([np.mean, np.size]).reset_index()\n",
    "    lat = aux[('lat', 'mean')]\n",
    "    lon = aux[('lon', 'mean')]\n",
    "    places = aux[('place_name', '')]\n",
    "    aux = pd.DataFrame()\n",
    "    aux['place_name'] = places\n",
    "    aux['lat'] = lat\n",
    "    aux['lon'] = lon\n",
    "    return aux\n",
    "#---------------------------------------------------------\n",
    "# AGG BUS STOPS\n",
    "#---------------------------------------------------------\n",
    "def aggBusStops(df):\n",
    "    infinite = 999999999999\n",
    "    busStops = pd.read_csv(\"./extra/paradas-de-colectivo.csv\", low_memory = False, sep=';')\n",
    "    busStops['X'] = changeStringListCommaForPoint(busStops['X'].tolist())\n",
    "    busStops['Y'] = changeStringListCommaForPoint(busStops['Y'].tolist())\n",
    "    aux = df\n",
    "    aux = getDataframeCoordsByPlaces(df)\n",
    "    aux = distanceAnalysis(aux, busStops, 'Y', 'X', 'distanceBusStops')\n",
    "    dicc = createDicc(aux['place_name'].tolist(), aux['distanceBusStops'].tolist())\n",
    "    \n",
    "    places = df['place_name'].tolist()\n",
    "    size = len(places)\n",
    "    distance = [None]*size\n",
    "    for i in range(0, size):\n",
    "        if places[i] not in dicc:\n",
    "            distance[i] = infinite\n",
    "        else:\n",
    "            distance[i] = dicc[places[i]]\n",
    "    df['distanceBusStops'] = distance\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# DELETE TRASH COLUMNS\n",
    "#---------------------------------------------------------\n",
    "def deleteTrashColumns(df):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if \"unnamed\" in column.lower():\n",
    "            df.drop(column, axis = 1, inplace = True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones del uso de informacion del campo de descripcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = 0\n",
    "true = 1\n",
    "#------------------------------------------------------\n",
    "# IS FLOAT\n",
    "#------------------------------------------------------\n",
    "def isFloat(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#------------------------------------------------------\n",
    "# INICIALIZAR DICCIONARIO\n",
    "#------------------------------------------------------\n",
    "# pre: Recibe una lista de claves\n",
    "# pos: devuelve un diccionario de esas claves recibidas inicializadas en cero\n",
    "\n",
    "def inicializar_diccionario(keys):\n",
    "    dicc = {}\n",
    "    for charac in keys:\n",
    "        dicc[charac] = 0\n",
    "    return dicc\n",
    "\n",
    "#------------------------------------------------------\n",
    "# ENCONTRAR FRASE\n",
    "#------------------------------------------------------\n",
    "# pre: Recibe un vector de palabras, la posicion en ese vector que se\n",
    "# esta leyendo, y un vector de frases\n",
    "# pos: Devuelve una tripla que dice si alguna de las frases en el\n",
    "# vector \"phrases\", el offset el cual debe desplazarse la posicion de lectura\n",
    "# del vector \"words\" y el indice en donde se encuantrala frase encontrada en el\n",
    "# vector \"phrases\"\n",
    "\n",
    "def encontrar_frase(words, i, phrases):\n",
    "    offset = 0\n",
    "    index = 0\n",
    "    for phrase in phrases:\n",
    "        if words[i].lower() in phrase:\n",
    "            phrase_split = phrase.split()\n",
    "            size = len(phrase_split)\n",
    "            offset = size\n",
    "            index = phrases.index(phrase)\n",
    "            for j in range(0, size):\n",
    "                if (i + j < size) and (words[i + j] != phrase_split[j]):\n",
    "                    return False, offset, index\n",
    "            return True, offset, index\n",
    "    return False, offset, index\n",
    "\n",
    "\n",
    "#------------------------------------------------------\n",
    "# CREAR DICCIONARIO DESCRIPCION\n",
    "#------------------------------------------------------\n",
    "# pre: Recibe un dataframe\n",
    "# pos: Devuelve una lista de diccionarios\n",
    "\n",
    "def crear_diccionario_descripcion(df):\n",
    "    characteristics = [\n",
    "        \"living\",\n",
    "        \"cochera\",\n",
    "        \"comedor\",\n",
    "        \"pileta\",\n",
    "        \"piscina\"\n",
    "    ]\n",
    "    phrases = [\n",
    "        \"cancha de tenis\",\n",
    "        \"club house\",\n",
    "        \"sector de juegos infantiles\",\n",
    "        \"futbol 5\",\n",
    "        \"seguridad las 24 hs\"\n",
    "    ]\n",
    "    size = len(df.index)\n",
    "    dicc_list = []\n",
    "    for i in range(0, size):\n",
    "        dicc = inicializar_diccionario(characteristics + phrases)\n",
    "        if 'description' not in df:\n",
    "            dicc_list.append(dicc)\n",
    "            continue\n",
    "        description = list(df['description'])\n",
    "        if type(description[i]) != type(\"\"):\n",
    "            dicc_list.append(dicc)\n",
    "            continue\n",
    "        words = description[i].split()\n",
    "        lenght = len(words)\n",
    "        for j in range(0, lenght):\n",
    "            (wordBelongs, offset, index) = encontrar_frase(words, j, phrases)\n",
    "            if wordBelongs:\n",
    "                j += offset\n",
    "                if j >= lenght:\n",
    "                    break\n",
    "                dicc[phrases[index]] = true\n",
    "            if words[j].lower() in characteristics:\n",
    "                dicc[words[j].lower()] = true\n",
    "        dicc_list.append(dicc)\n",
    "    return dicc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de filtrado del data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# FILTER PORCENTAGE\n",
    "#----------------------------------------------------------------------\n",
    "def filterPercentage(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if priceUSD <= 0:\n",
    "        return np.nan\n",
    "    price = usdM2 * surfaceTotal\n",
    "    dif = abs(price - priceUSD)\n",
    "    if ((dif / priceUSD) * 100) <= 10:\n",
    "        return 1\n",
    "    return np.nan\n",
    "#----------------------------------------------------------------------\n",
    "# FILTER IMPOSSIBLES\n",
    "#----------------------------------------------------------------------\n",
    "def filterImposibles(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if (np.isnan(surfaceTotal) or surfaceTotal <= 0) and (not np.isnan(priceUSD) and not np.isnan(usdM2)):\n",
    "        return 1\n",
    "    if (np.isnan(priceUSD) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(usdM2)):\n",
    "        return 1\n",
    "    if (np.isnan(usdM2) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(priceUSD)):\n",
    "        return 1\n",
    "    if (not np.isnan(usdM2) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(priceUSD)):\n",
    "        return 1\n",
    "    return np.nan\n",
    "#----------------------------------------------------------------------\n",
    "# FILL PRICE\n",
    "#----------------------------------------------------------------------\n",
    "def fillPrice(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if np.isnan(priceUSD) and not np.isnan(usdM2):\n",
    "        return (usdM2 * surfaceTotal)\n",
    "    return priceUSD\n",
    "#----------------------------------------------------------------------\n",
    "# FILL M2\n",
    "#----------------------------------------------------------------------\n",
    "def fillM2(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if surfaceTotal <= 0:\n",
    "        return np.nan\n",
    "    if not np.isnan(priceUSD) and np.isnan(usdM2):\n",
    "        return (priceUSD / surfaceTotal)\n",
    "    return usdM2\n",
    "#----------------------------------------------------------------------\n",
    "# FILL SURFACE\n",
    "#----------------------------------------------------------------------\n",
    "def fillSurface(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if not np.isnan(priceUSD) and np.isnan(usdM2):\n",
    "        return (priceUSD / usdM2)\n",
    "    return surfaceTotal\n",
    "#----------------------------------------------------------------------\n",
    "# Obtenemos el año y mes del nombre de archivo\n",
    "#----------------------------------------------------------------------\n",
    "def addDate(date, df):\n",
    "    date_splitted = archive.split('-')\n",
    "    month = date_splitted[3]\n",
    "    year = date_splitted[2]\n",
    "    date = int(year + month)\n",
    "    size = len(df.index)\n",
    "    dates = pd.Series([date for i in range(0, size)])\n",
    "    # y lo ponemos como dato en una columna\n",
    "    df['date'] = dates\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# Durante la carga de datos, se eliminan ciertas columnas que nos \n",
    "#resultan irrelevantes para el trabajo\n",
    "#----------------------------------------------------------------------\n",
    "def filterUnnecesaryColumns(df, deleteId):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if column in columnsToUseInitialy:\n",
    "            continue\n",
    "        if not deleteId and column == 'id':\n",
    "            continue\n",
    "        df.drop(column, axis = 1, inplace = True)\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# ADD DESCRIPTIONS COLUMNS\n",
    "#----------------------------------------------------------------------\n",
    "def addDescriptionColumns(df, columDict):\n",
    "    size = len(df.index)\n",
    "    description = list(df[columDict])\n",
    "    keys = description[0].keys()\n",
    "    for key in keys:\n",
    "        colum = []\n",
    "        for i in range(0, size):\n",
    "            value = description[i][key]\n",
    "            colum.append(value)\n",
    "        df[key] = colum\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# CHANGE PLACE WITH PARENT NAMES COLUMN TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def changePlaceWithParentsNamesColumn(df):\n",
    "    if 'place_with_parent_names' not in df:\n",
    "        return df\n",
    "    listPlaces = df['place_with_parent_names'].tolist()\n",
    "    size = len(listPlaces)\n",
    "    for i in range(0, size):\n",
    "        listPlaces[i] = PlaceToNumber(listPlaces[i])\n",
    "    df['place_with_parent_names'] = listPlaces\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# PLACE TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def PlaceToNumber(x):\n",
    "    CF = 0\n",
    "    GBA = 1\n",
    "    x = str(x)\n",
    "    if 'Capital Federal' in x:\n",
    "        return CF\n",
    "    else:\n",
    "        return GBA\n",
    "#----------------------------------------------------------------------\n",
    "# CHANGE PROPERTY TYPE COLUMN TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def changePropertyTypeColumn(df):    \n",
    "    listPropertyType = df['property_type'].tolist()\n",
    "    size = len(listPropertyType)\n",
    "    for i in range(0, size):\n",
    "        listPropertyType[i] = propertyTypeToNumber(listPropertyType[i])\n",
    "    df['property_type'] = listPropertyType\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# PROPERTY TYPE TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def propertyTypeToNumber(x):\n",
    "    ph = 0\n",
    "    apartment = 1\n",
    "    house = 2\n",
    "    store = 3\n",
    "    if x.lower() == \"apartment\" or x.lower() == \"departamento\":\n",
    "        return apartment\n",
    "    if x.lower() == \"ph\":\n",
    "        return ph\n",
    "    if x.lower() == \"house\" or x.lower() == \"casa\":\n",
    "        return house\n",
    "    if x.lower() == \"store\":\n",
    "        return store\n",
    "#----------------------------------------------------------------------\n",
    "# AGG FLOOR\n",
    "#----------------------------------------------------------------------\n",
    "def aggFloor(floor):\n",
    "    if np.isnan(floor):\n",
    "        return 0\n",
    "    else:\n",
    "        return floor\n",
    "#------------------------------------------------------\n",
    "# CREATE DICC\n",
    "#------------------------------------------------------\n",
    "def createDicc(keys, values):\n",
    "    dicc = {}\n",
    "    size = len(keys)\n",
    "    for i in range(0, size):\n",
    "        dicc[keys[i]] = values[i]\n",
    "    return dicc\n",
    "\n",
    "#------------------------------------------------------\n",
    "# GET AVG OF VALUES\n",
    "#------------------------------------------------------\n",
    "def getAvgOfValues(df, values):\n",
    "    result = []\n",
    "    for column in values:\n",
    "        valuesList = df[column].tolist()\n",
    "        valuesList = [x for x in valuesList if (not np.isnan(x) and x != 0)]\n",
    "        size = len(valuesList)\n",
    "        if size == 0:\n",
    "            df[column] = np.nan\n",
    "            continue\n",
    "        valuesList.sort()\n",
    "        df[column] = valuesList[size/2]\n",
    "    return df\n",
    "#------------------------------------------------------\n",
    "# GET AVG VALUES BY PLACE\n",
    "#------------------------------------------------------\n",
    "def getAvgValuesBykey(df, values, key):\n",
    "    aux = df\n",
    "    aux = aux.loc[:, values+[key]]\\\n",
    "    .groupby(key).apply(lambda x: getAvgOfValues(x, values)).reset_index()\n",
    "    for column in values:\n",
    "        valuesList = aux[column].tolist()\n",
    "        keyList = aux[key].tolist()\n",
    "        dicc = createDicc(keyList, valuesList)\n",
    "\n",
    "        valuesList = df[column].tolist()\n",
    "        keyList = df[key].tolist()\n",
    "        size = len(valuesList)\n",
    "        for i in range(0, size):\n",
    "            if not np.isnan(valuesList[i]) and valuesList[i] != 0:\n",
    "                continue\n",
    "            ## Caso especial donde Buenos Aire interior \n",
    "            ## hay solo una propiedad con todo en nan\n",
    "            if key == 'state_name' and keyList[i] == 'Buenos Aires Interior':\n",
    "                avgGBAO = dicc['Bs.As. G.B.A. Zona Oeste']\n",
    "                avgGBAS = dicc['Bs.As. G.B.A. Zona Sur']\n",
    "                avgBAI = (avgGBAO + avgGBAS) / 2\n",
    "                valuesList[i] = avgBAI\n",
    "            else:\n",
    "                valuesList[i] = dicc[keyList[i]]\n",
    "        df[column] = valuesList\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop principal del filtrado de cada dataframe del set de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte del código se filtraron columnas de los archivos csv que no ibamos a a usar. Tambiénse filtraron propiedades que no pertenezcan a Capital Federal o Gran Buenos Aires, ya que solo vamos a acotar nuestro análisis a estos dos lugares. Tambien fué necesario renombrar la columna de superficie ya que algunos archivos CSV tenian nombres distintos y es necesario que todos refieran al mismo.\n",
    "Luego agregamos la columna llamada Date la cual refiere a la fecha en la cual fue publicada cada propiedad. Se uso esta fecha ya que la columna 'created_on' no refleja la evolucion del precio de las propiedades en funcion del tiempo y distorciona el analisis de datos. En cambio la fecha de publicacion muestra la actualizacion a la fecha de los precios de cada propiedad.\n",
    "\n",
    "Tambien se filtraron porpiedades en función de la validez de sus datos. Es decir, que si el valor en dolares por metro cuadradro multiplicado por la superficie no se encontraba en un rango menor al 10% respecto del precio en dolares, se descartaban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properati-AR-2017-05-01-properties-sell.csv\n",
      "properati-AR-2014-01-01-properties-sell.csv\n",
      "properati-AR-2013-08-01-properties-sell.csv\n",
      "properati-AR-2015-04-01-properties-sell.csv\n",
      "properati-AR-2014-02-01-properties-sell.csv\n",
      "properati-AR-2013-09-01-properties-sell.csv\n",
      "properati-AR-2016-09-01-properties-sell.csv\n",
      "properati-AR-2017-01-01-properties-sell.csv\n",
      "properati-AR-2013-10-01-properties-sell.csv\n",
      "properati-AR-2015-05-01-properties-sell.csv\n",
      "properati-AR-2013-12-01-properties-sell.csv\n",
      "properati-AR-2014-05-01-properties-sell.csv\n",
      "properati-AR-2014-11-01-properties-sell.csv\n",
      "properati-AR-2014-09-01-properties-sell.csv\n",
      "properati-AR-2015-02-01-properties-sell.csv\n",
      "properati-AR-2016-08-01-properties-sell.csv\n",
      "properati-AR-2016-01-01-properties-sell.csv\n",
      "properati-AR-2016-11-01-properties-sell.csv\n",
      "properati-AR-2015-08-01-properties-sell.csv\n",
      "properati-AR-2017-03-01-properties-sell.csv\n",
      "properati-AR-2014-12-01-properties-sell.csv\n",
      "properati-AR-2015-12-01-properties-sell.csv\n",
      "properati-AR-2016-06-01-properties-sell.csv\n",
      "properati-AR-2015-06-01-properties-sell.csv\n",
      "properati-AR-2014-08-01-properties-sell.csv\n",
      "properati-AR-2016-04-01-properties-sell.csv\n",
      "properati-AR-2015-01-01-properties-sell.csv\n",
      "properati-AR-2014-06-01-properties-sell.csv\n",
      "properati-AR-2017-02-01-properties-sell.csv\n",
      "properati-AR-2013-11-01-properties-sell.csv\n",
      "properati-AR-2016-05-01-properties-sell.csv\n",
      "properati-AR-2015-03-01-properties-sell.csv\n",
      "properati-AR-2016-07-01-properties-sell.csv\n",
      "properati-AR-2014-07-01-properties-sell.csv\n",
      "properati-AR-2014-10-01-properties-sell.csv\n",
      "properati-AR-2015-11-01-properties-sell.csv\n",
      "properati-AR-2014-04-01-properties-sell.csv\n",
      "properati-AR-2017-06-06-properties-sell.csv\n",
      "properati-AR-2014-03-01-properties-sell.csv\n",
      "properati-AR-2017-06-01-properties-sell.csv\n",
      "properati-AR-2016-12-01-properties-sell.csv\n",
      "properati-AR-2015-10-01-properties-sell.csv\n",
      "properati-AR-2015-07-01-properties-sell.csv\n",
      "properati-AR-2015-09-01-properties-sell.csv\n",
      "properati-AR-2016-03-01-properties-sell.csv\n",
      "properati-AR-2017-04-01-properties-sell.csv\n",
      "properati-AR-2017-07-03-properties-sell.csv\n",
      "properati-AR-2016-02-01-properties-sell.csv\n",
      "properati-AR-2016-10-01-properties-sell.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Ruta de la carpeta con los archivos de datos modificados\n",
    "root = \"./properties/\"\n",
    "indexAcum = 0\n",
    "for archive in listdir(root):\n",
    "    if \".csv\" not in archive:\n",
    "        continue\n",
    "    df = pd.read_csv(root + archive, low_memory = False)\n",
    "    \n",
    "    df = df.loc[df.place_with_parent_names.str.contains('Capital Federal') \\\n",
    "        | df.place_with_parent_names.str.contains('Bs.As. G.B.A.'), :]\n",
    "    \n",
    "    # En algunos casos, es necesario renombrar algunas columnas\n",
    "    if 'price_aprox_usd' not in df:\n",
    "        df.rename(columns = {'price': 'price_aprox_usd'}, inplace = True)\n",
    "    if 'surface_total_in_m2' not in df:\n",
    "        df.rename(columns = {'surface_in_m2': 'surface_total_in_m2'}, \\\n",
    "            inplace = True)\n",
    "    if 'surface_covered_in_m2' not in df and 'surface_total_in_m2' in df:\n",
    "        df['surface_covered_in_m2'] = df['surface_total_in_m2'].tolist()\n",
    "\n",
    "    # Durante la carga de datos, se eliminan ciertas columnas que nos \n",
    "    # resultan irrelevantes para el trabajo.\n",
    "    df = filterUnnecesaryColumns(df, True)\n",
    "        \n",
    "    # Aquí reconvertimos algunas columnas a punto flotante\n",
    "    df.loc[:, 'price_aprox_usd'] = df.loc[:, ['price_aprox_usd']]\\\n",
    "    .apply(lambda x: float(x), axis = 1)\n",
    "    df.loc[:, 'price_usd_per_m2'] = df.loc[:, ['price_usd_per_m2']]\\\n",
    "    .apply(lambda x: float(x), axis = 1)\n",
    "\n",
    "    # Obtenemos el año y mes del nombre de archivo\n",
    "    df = addDate(archive, df)\n",
    "    \n",
    "    # Aquí aplicamos el filtro antes declarado\n",
    "    df['filter1'] = df.loc[:, ['price_aprox_usd', 'price_usd_per_m2', \\\n",
    "    'surface_total_in_m2']].apply(lambda x: filterImposibles(x), axis = 1)\n",
    "    df = df[df['filter1'] == 1]\n",
    "    df.drop('filter1', axis = 1, inplace = True)\n",
    "    \n",
    "    size = len(df.index)\n",
    "    if size == 0:\n",
    "        continue\n",
    "    \n",
    "    #print \"columns: \", list(df.columns.values)\n",
    "    \n",
    "    df.loc[:, ['price_aprox_usd']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillPrice(x), axis = 1)\n",
    "    \n",
    "    df.loc[:, ['price_usd_per_m2']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillM2(x), axis = 1)\n",
    "    \n",
    "    df.loc[:, ['surface_total_in_m2']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillSurface(x), axis = 1)\n",
    "    \n",
    "    df['filter2'] = df.loc[:, ['price_aprox_usd', 'price_usd_per_m2', \\\n",
    "    'surface_total_in_m2']].apply(lambda x: filterPercentage(x), axis = 1)\n",
    "    df = df[df['filter2'] == 1]\n",
    "    df.drop('filter2', axis = 1, inplace = True)\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    df.loc[:, ['place_with_parent_names']] = df.loc[:, ['place_with_parent_names']]\\\n",
    "    .apply(lambda x: PlaceToNumber(x), axis = 1)\n",
    "    df = changePropertyTypeColumn(df)\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    # Si el filtrado es tal que me quedo sin dataframe, \n",
    "    # entonces salto a la siguiente iteracion\n",
    "    size = len(df.index)\n",
    "    if size == 0:\n",
    "        continue\n",
    "    \n",
    "    #Obtengo los campos de descripcion\n",
    "    df['rooms'] = getRooms(df)\n",
    "    df = getAvgValuesBykey(df, ['rooms'], 'place_name')\n",
    "    df['description'] = crear_diccionario_descripcion(df)\n",
    "    df = addDescriptionColumns(df, 'description')\n",
    "    df.drop('description', axis = 1, inplace = True)\n",
    "    \n",
    "    # Borro registros con algun nan\n",
    "    df.dropna(axis=0, how='any', subset=list(df.columns.values), inplace=True)\n",
    "    \n",
    "    # Agrego distancias\n",
    "    df = aggSubways(df)\n",
    "    df = aggUniversities(df)\n",
    "    df = aggHospitales(df)\n",
    "    df = aggBusStops(df)\n",
    "    \n",
    "    # Finalmente, guardamos los archivos modificados.\n",
    "    size = len(df.index)\n",
    "    newIndex = [i for i in range(0, size)]\n",
    "    df.reindex(newIndex)\n",
    "    print archive\n",
    "    df.to_csv(\"./PreProccess/\"+archive, index = True, header = True, \\\n",
    "        sep = ',', encoding = 'utf-8-sig')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concateno los archivos del set de entrenamiento previamente filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "properties = []\n",
    "root = \"./PreProccess/\"\n",
    "for archive in listdir(root):\n",
    "    if \".csv\" not in archive:\n",
    "        continue\n",
    "    df = pd.read_csv(root + archive, low_memory = False)\n",
    "    properties.append(df)\n",
    "    \n",
    "general = pd.concat(properties)\n",
    "general = deleteTrashColumns(general)\n",
    "#Grabo la concatenacion en un unico csv\n",
    "try:\n",
    "    general.to_csv(\"propertiesTrain.csv\", index = True, header = True, \\\n",
    "        sep = ',', encoding = 'utf-8-sig')\n",
    "    print('Done')\n",
    "except value:\n",
    "    print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de filtrado y acomodamiento de los datos para entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones las uso para filtrar datos especificos del set de etrenamiento que ya fue concatenado, y ademas filtramos el set de datos con las funciones que tenemos a continuacion y con las funciones previamente declaradas para quedarnos con las columnas a evaluear. Tener en cunta que como el set de datos a predecir tiene campos importantes como la superficie, barrios, etc, con algunos Nan, es necesario completar estos con datos propios del mismo set de datos. Esto se puede lograr mediante el campo de desciripcion qur tiene informacion, per en muchos caso la informacion del campo de descripcion no es fiables, entonces se usa por ejempli para la superfiie, la mediana de cada barrio de manera de equilibrar los valores intermedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# FUNCTIONS OF PROCCES OF TRAIN DATA\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# HASH PLACES\n",
    "#---------------------------------------------------------\n",
    "def hashPlaces(df, predictDf):\n",
    "    placesHash = {}\n",
    "    groupedPlaces = df.loc[:, ['place_name', 'floor']]\\\n",
    "    .groupby('place_name').agg([np.size]).reset_index()\n",
    "    places = groupedPlaces['place_name'].tolist()\n",
    "    groupedPlaces = predictDf.loc[:, ['place_name', 'floor']]\\\n",
    "    .groupby('place_name').agg([np.size]).reset_index()\n",
    "    places = places + groupedPlaces['place_name'].tolist()\n",
    "    size = len(places)\n",
    "    for i in range(0, size):\n",
    "        if places[i] in placesHash:\n",
    "            continue\n",
    "        placesHash[places[i]] = 0\n",
    "    keys = placesHash.keys()\n",
    "    size = len(keys)\n",
    "    for i in range(0, size):\n",
    "        placesHash[keys[i]] = i\n",
    "    placesHash[\"no place\"] = size\n",
    "    return placesHash\n",
    "#---------------------------------------------------------\n",
    "# PROCCES NO PLACES\n",
    "#---------------------------------------------------------\n",
    "def processNoPlaces(df):\n",
    "    places = df['place_name'].tolist()\n",
    "    size = len(places)\n",
    "    for i in range(0, size):\n",
    "        if type(places[i]) != type(\"\"):\n",
    "            places[i] = \"no place\"\n",
    "    df['place_name'] = places\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# CNVERT PLACES TO HASH NUMBER\n",
    "#---------------------------------------------------------\n",
    "def convertPlacesToHashNumber(df, placesHash):   \n",
    "    placesDataTrain = df['place_name'].tolist()\n",
    "    size = len(placesDataTrain)\n",
    "    count = 0\n",
    "    hashSize = len(placesHash.keys())\n",
    "    for i in range(0, size):\n",
    "        placeName = placesDataTrain[i]\n",
    "        placesDataTrain[i] = placesHash[placeName]\n",
    "    df['place_name'] = placesDataTrain\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# DELETE EXTRA COLUMNS\n",
    "#---------------------------------------------------------\n",
    "def deleteExtraColumns(df):\n",
    "    extra = [\n",
    "        #'sector de juegos infantiles',\n",
    "        #'seguridad las 24 hs',\n",
    "        #'cancha de tenis',\n",
    "        #'club house',\n",
    "        #'cochera',\n",
    "        #'comedor',\n",
    "        #'futbol 5',\n",
    "        #'living',\n",
    "        #'pileta',\n",
    "        #'piscina',\n",
    "        #'rooms',\n",
    "        #'place_with_parent_names',\n",
    "        'date',\n",
    "        #'surface_covered_in_m2',\n",
    "        #'distanceBusStops',\n",
    "        #'distanceHospitales',\n",
    "        #'distanceSubway',\n",
    "        #'distanceUniversities',\n",
    "    ]\n",
    "    columns = [\n",
    "        'Unnamed: 0',  \n",
    "        'price_usd_per_m2',\n",
    "    ]\n",
    "    df.drop(columns+extra, axis = 1, inplace = True)\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if \"unnamed\" in column.lower():\n",
    "            df.drop(column, axis = 1, inplace = True) \n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# DELETE TRASH COLUMNS\n",
    "#---------------------------------------------------------\n",
    "def deleteTrashColumns(df):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if \"unnamed\" in column.lower():\n",
    "            df.drop(column, axis = 1, inplace = True) \n",
    "    return df\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# FUNCTIONS OF PROCCES OF TEST DATA\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# DELETE COLUMNS TEST DATA\n",
    "#---------------------------------------------------------\n",
    "def deleteColumnsTestData(df, columnsToEvaluate):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if column in columnsToEvaluate:\n",
    "            continue\n",
    "        df.drop(column, axis = 1, inplace = True)\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# PRE PROCCES TEST DATA\n",
    "#---------------------------------------------------------\n",
    "def preProcessTestData(df, placesHash):\n",
    "    df['description'] = crear_diccionario_descripcion(df)\n",
    "    df = processNoPlaces(df)\n",
    "    df = convertPlacesToHashNumber(df, placesHash)\n",
    "    df = addDescriptionColumns(df, 'description')\n",
    "    df.drop('description', axis = 1, inplace = True)\n",
    "    df = changePlaceWithParentsNamesColumn(df)\n",
    "    df = changePropertyTypeColumn(df)\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# FILTER BY DATE\n",
    "#---------------------------------------------------------\n",
    "def filterByDate(df):\n",
    "    dates = df['date'].tolist()\n",
    "    size = len(dates)\n",
    "    for i in range(0, size):\n",
    "        if dates[i]/100 < 2016:\n",
    "            dates[i] = 0\n",
    "    df['date'] = dates\n",
    "    df = df[df['date'] != 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Second filter proccess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"propertiesTrain.csv\", low_memory = False)\n",
    "df = filterByDate(df)\n",
    "predictDf = pd.read_csv(\"properati_dataset_testing_noprice.csv\", low_memory = False)\n",
    "df = deleteExtraColumns(df)\n",
    "df.dropna(axis=0, how='any', subset=list(df.columns.values), inplace=True)\n",
    "placesHash = hashPlaces(df, predictDf)\n",
    "df = processNoPlaces(df)\n",
    "df = convertPlacesToHashNumber(df, placesHash)\n",
    "df.to_csv(\"dataTrain.csv\", index = True, header = True, sep = ',', \n",
    "          encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataTrain.csv\", low_memory = False)\n",
    "df = deleteTrashColumns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column size:  23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cancha de tenis',\n",
       " 'club house',\n",
       " 'cochera',\n",
       " 'comedor',\n",
       " 'distanceBusStops',\n",
       " 'distanceHospitales',\n",
       " 'distanceSubway',\n",
       " 'distanceUniversities',\n",
       " 'futbol 5',\n",
       " 'lat',\n",
       " 'living',\n",
       " 'lon',\n",
       " 'pileta',\n",
       " 'piscina',\n",
       " 'place_name',\n",
       " 'place_with_parent_names',\n",
       " 'price_aprox_usd',\n",
       " 'property_type',\n",
       " 'rooms',\n",
       " 'sector de juegos infantiles',\n",
       " 'seguridad las 24 hs',\n",
       " 'surface_covered_in_m2',\n",
       " 'surface_total_in_m2']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsToEvaluate = list(df.columns.values)\n",
    "print \"column size: \", len(columnsToEvaluate)\n",
    "columnsToEvaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProccess data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictDf = pd.read_csv(\"properati_dataset_testing_noprice.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "predictDf = getAvgValuesBykey(predictDf, ['lat', 'lon'], 'place_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['lat', 'lon'], 'state_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['surface_total_in_m2'], 'place_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['surface_total_in_m2'], 'state_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['surface_covered_in_m2'], 'place_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['surface_covered_in_m2'], 'state_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['rooms'], 'place_name')\n",
    "predictDf = getAvgValuesBykey(predictDf, ['rooms'], 'state_name')\n",
    "predictDf = aggSubways(predictDf)\n",
    "predictDf = aggUniversities(predictDf)\n",
    "predictDf = aggHospitales(predictDf)\n",
    "predictDf  = aggBusStops(predictDf)\n",
    "columnsToEvaluate = list(df.columns.values)\n",
    "columnsToEvaluate.remove('price_aprox_usd')\n",
    "predictDf['date'] = [201708 for i in range(0, len(predictDf.index))]\n",
    "predictDf = deleteColumnsTestData(predictDf, columnsToEvaluate+['id', 'description'])\n",
    "predictDf = preProcessTestData(predictDf, placesHash)\n",
    "predictDf = deleteColumnsTestData(predictDf, columnsToEvaluate+['id', 'description'])\n",
    "predictDf.to_csv(\"dataTest.csv\", index = True, header = True, sep = ',', \n",
    "          encoding = 'utf-8-sig')\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column size:  23\n"
     ]
    }
   ],
   "source": [
    "columnsDataTest = list(predictDf.columns.values)\n",
    "print \"column size: \", len(columnsDataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
