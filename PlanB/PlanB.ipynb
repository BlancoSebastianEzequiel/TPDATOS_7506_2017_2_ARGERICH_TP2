{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproceso del set de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones del uso de informacion del campo de descripcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFloat(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = 0\n",
    "true = 1\n",
    "# ------------------------------------------------------------------------------\n",
    "# INICIALIZAR DICCIONARIO\n",
    "# ------------------------------------------------------------------------------\n",
    "# pre: Recibe una lista de claves\n",
    "# pos: devuelve un diccionario de esas claves recibidas inicializadas en cero\n",
    "\n",
    "def inicializar_diccionario(keys):\n",
    "    dicc = {}\n",
    "    for charac in keys:\n",
    "        dicc[charac] = 0\n",
    "    return dicc\n",
    "\n",
    "#------------------------------------------------------\n",
    "# GET SURFACE\n",
    "#------------------------------------------------------\n",
    "def getSurface(df):\n",
    "    surfaces = df['surface_total_in_m2'].tolist()\n",
    "    if 'description' not in df:\n",
    "        return surfaces\n",
    "    dfSize = len(df.index)\n",
    "    descriptions = df['description'].tolist()\n",
    "    for i in range(0, dfSize):\n",
    "        if type(descriptions[i]) != type(\"\"):\n",
    "            validateSurface(surfaces[i], 0)\n",
    "            continue\n",
    "        surfaceCalculated = 0\n",
    "        words = descriptions[i].split()\n",
    "        wordsSize = len(words)\n",
    "        for pos in range(0, wordsSize):\n",
    "            triple = findSurface(words, pos, surfaceCalculated)\n",
    "            surfaceCalculated, offset, iFound = triple\n",
    "            if iFound:\n",
    "                pos += offset\n",
    "        surfaces[i] = validateSurface(surfaces[i], surfaceCalculated)\n",
    "    return surfaces\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# VALIDATE SURFACE\n",
    "# ------------------------------------------------------------------------------\n",
    "def validateSurface(surface, surfaceCalculated):\n",
    "    if np.isnan(surface):\n",
    "        return surfaceCalculated\n",
    "    if surface != surfaceCalculated:\n",
    "        return surface\n",
    "    if surface == surfaceCalculated:\n",
    "        return surface\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "# ENCONTRAR SUPERFICIE\n",
    "# ------------------------------------------------------------------------------\n",
    "def findSurface(words, i, surface):\n",
    "    size = len(words)\n",
    "    offset = 0 \n",
    "    iFound = False\n",
    "    if isFloat(words[i]):\n",
    "        if (i + 2 < size) and words[i+1].lower() == \"x\" and isFloat(words[i+2]):\n",
    "            a = float(words[i])\n",
    "            b = float(words[i+2])        \n",
    "            surface += a*b\n",
    "            return surface, offset, iFound\n",
    "    return surface, offset, iFound\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ENCONTRAR CANTIDAD DE AMBIENTES\n",
    "# ------------------------------------------------------------------------------\n",
    "def FindsNumberOfRooms(words, i, numberOfRooms):\n",
    "    rooms = [\n",
    "        \"living\", \n",
    "        \"comedor\", \n",
    "        \"cocina\", \n",
    "        \"lavadero\", \n",
    "        \"dormitorio\", \n",
    "        \"vestidor\",\n",
    "        \"baÃ±o\"\n",
    "    ]\n",
    "    end = False\n",
    "    size = len(words)\n",
    "    if words[i].isdigit():\n",
    "        if (i + 1 < size) and words[i+1].lower() == \"ambientes\":\n",
    "            numberOfRooms = int(words[i])\n",
    "            return numberOfRooms, end\n",
    "        if (i + 1 < size) and words[i+1].lower() == \"dormitorios\":\n",
    "            numberOfRooms += int(words[i])\n",
    "            return numberOfRooms, not end\n",
    "    if words[i].lower in rooms:\n",
    "        numberOfRooms += 1\n",
    "        return numberOfRooms, not end\n",
    "    return numberOfRooms, not end\n",
    "\n",
    "#------------------------------------------------------\n",
    "# VALIDATE ROOM\n",
    "#------------------------------------------------------\n",
    "def validateRoom(numberOfRooms, numberOfRoomsCalculated):\n",
    "    if np.isnan(numberOfRooms):\n",
    "        return numberOfRoomsCalculated\n",
    "    if numberOfRooms != numberOfRoomsCalculated:\n",
    "        return numberOfRooms\n",
    "    if numberOfRooms == numberOfRoomsCalculated:\n",
    "        return numberOfRooms\n",
    "    \n",
    "#------------------------------------------------------\n",
    "# GET ROOMS\n",
    "#------------------------------------------------------\n",
    "def getRooms(df):\n",
    "    rooms = df['rooms'].tolist()\n",
    "    if 'description' not in df:\n",
    "        return rooms\n",
    "    dfSize = len(df.index)\n",
    "    descriptions = df['description'].tolist()\n",
    "    for i in range(0, dfSize):\n",
    "        if type(descriptions[i]) != type(\"\"):\n",
    "            validateRoom(rooms[i], 0)\n",
    "            continue\n",
    "        numberOfRooms = 0\n",
    "        words = descriptions[i].split()\n",
    "        wordsSize = len(words)\n",
    "        for pos in range(0, wordsSize):\n",
    "            numberOfRooms, end = FindsNumberOfRooms(words, pos, numberOfRooms)\n",
    "            if end:\n",
    "                break\n",
    "        rooms[i] = validateRoom(rooms[i], numberOfRooms)\n",
    "    return rooms\n",
    "# ------------------------------------------------------------------------------\n",
    "# ENCONTRAR FRASE\n",
    "# ------------------------------------------------------------------------------\n",
    "# pre: Recibe un vector de palabras, la posicion en ese vector que se\n",
    "# esta leyendo, y un vector de frases\n",
    "# pos: Devuelve una tripla que dice si alguna de las frases en el\n",
    "# vector \"phrases\", el offset el cual debe desplazarse la posicion de lectura\n",
    "# del vector \"words\" y el indice en donde se encuantrala frase encontrada en el\n",
    "# vector \"phrases\"\n",
    "\n",
    "def encontrar_frase(words, i, phrases):\n",
    "    offset = 0\n",
    "    index = 0\n",
    "    for phrase in phrases:\n",
    "        if words[i].lower() in phrase:\n",
    "            phrase_split = phrase.split()\n",
    "            size = len(phrase_split)\n",
    "            offset = size\n",
    "            index = phrases.index(phrase)\n",
    "            for j in range(0, size):\n",
    "                if (i + j < size) and (words[i + j] != phrase_split[j]):\n",
    "                    return False, offset, index\n",
    "            return True, offset, index\n",
    "    return False, offset, index\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CREAR DICCIONARIO DESCRIPCION\n",
    "# ------------------------------------------------------------------------------\n",
    "# pre: Recibe un dataframe\n",
    "# pos: Devuelve una lista de diccionarios\n",
    "\n",
    "def crear_diccionario_descripcion(df):\n",
    "    characteristics = [\n",
    "        \"living\",\n",
    "        \"cochera\",\n",
    "        \"comedor\",\n",
    "        \"pileta\",\n",
    "        \"piscina\"\n",
    "    ]\n",
    "    phrases = [\n",
    "        \"cancha de tenis\",\n",
    "        \"club house\",\n",
    "        \"sector de juegos infantiles\",\n",
    "        \"futbol 5\",\n",
    "        \"seguridad las 24 hs\"\n",
    "    ]\n",
    "    size = len(df.index)\n",
    "    dicc_list = []\n",
    "    for i in range(0, size):\n",
    "        dicc = inicializar_diccionario(characteristics + phrases)\n",
    "        if 'description' not in df:\n",
    "            dicc_list.append(dicc)\n",
    "            continue\n",
    "        description = list(df['description'])\n",
    "        if type(description[i]) != type(\"\"):\n",
    "            dicc_list.append(dicc)\n",
    "            continue\n",
    "        words = description[i].split()\n",
    "        lenght = len(words)\n",
    "        for j in range(0, lenght):\n",
    "            (wordBelongs, offset, index) = encontrar_frase(words, j, phrases)\n",
    "            if wordBelongs:\n",
    "                j += offset\n",
    "                if j >= lenght:\n",
    "                    break\n",
    "                dicc[phrases[index]] = true\n",
    "            if words[j].lower() in characteristics:\n",
    "                dicc[words[j].lower()] = true\n",
    "        dicc_list.append(dicc)\n",
    "    return dicc_list\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de filtrado del data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Filtro de propiedades con precio calculable. Devuelve 1 si es vÃ¡lido. \n",
    "# De lo contrario, nan (Not A Number)\n",
    "#----------------------------------------------------------------------\n",
    "def filterPercentage(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if priceUSD <= 0:\n",
    "        return np.nan\n",
    "    price = usdM2 * surfaceTotal\n",
    "    dif = abs(price - priceUSD)\n",
    "    if ((dif / priceUSD) * 100) <= 10:\n",
    "        return 1\n",
    "    return np.nan\n",
    "#----------------------------------------------------------------------\n",
    "# Filtro de propiedades con precio calculable. Devuelve 1 si es vÃ¡lido. \n",
    "# De lo contrario, nan (Not A Number)\n",
    "#----------------------------------------------------------------------\n",
    "def filterImposibles(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if (np.isnan(surfaceTotal) or surfaceTotal <= 0) and (not np.isnan(priceUSD) and not np.isnan(usdM2)):\n",
    "        return 1\n",
    "    if (np.isnan(priceUSD) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(usdM2)):\n",
    "        return 1\n",
    "    if (np.isnan(usdM2) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(priceUSD)):\n",
    "        return 1\n",
    "    if (not np.isnan(usdM2) and (not np.isnan(surfaceTotal) or surfaceTotal > 0)) and (not np.isnan(priceUSD)):\n",
    "        return 1\n",
    "    return np.nan\n",
    "#----------------------------------------------------------------------\n",
    "# CÃ¡lculo del precio aproximado de venta\n",
    "#----------------------------------------------------------------------\n",
    "def fillPrice(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if np.isnan(priceUSD) and not np.isnan(usdM2):\n",
    "        return (usdM2 * surfaceTotal)\n",
    "    return priceUSD\n",
    "#----------------------------------------------------------------------\n",
    "# CÃ¡lculo del precio del metro cuadrado\n",
    "#----------------------------------------------------------------------\n",
    "def fillM2(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if surfaceTotal <= 0:\n",
    "        return np.nan\n",
    "    if not np.isnan(priceUSD) and np.isnan(usdM2):\n",
    "        return (priceUSD / surfaceTotal)\n",
    "    return usdM2\n",
    "#----------------------------------------------------------------------\n",
    "# CÃ¡lculo de la superficie\n",
    "#----------------------------------------------------------------------\n",
    "def fillSurface(array):\n",
    "    priceUSD, usdM2, surfaceTotal = array\n",
    "    if not np.isnan(priceUSD) and np.isnan(usdM2):\n",
    "        return (priceUSD / usdM2)\n",
    "    return surfaceTotal\n",
    "#----------------------------------------------------------------------\n",
    "# Obtenemos el aÃ±o y mes del nombre de archivo\n",
    "#----------------------------------------------------------------------\n",
    "def addDate(date, df):\n",
    "    date_splitted = archive.split('-')\n",
    "    month = date_splitted[3]\n",
    "    year = date_splitted[2]\n",
    "    date = year + '-' + month\n",
    "    size = len(df.index)\n",
    "    dates = pd.Series([date for i in range(0, size)])\n",
    "    # y lo ponemos como dato en una columna\n",
    "    df['date'] = dates\n",
    "    df.loc[:, ['date']] = pd.to_datetime(df['date'], errors = 'coerce')\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# Durante la carga de datos, se eliminan ciertas columnas que nos \n",
    "#resultan irrelevantes para el trabajo\n",
    "#----------------------------------------------------------------------\n",
    "def filterUnnecesaryColumns(df, isId):\n",
    "    if 'surface_covered_in_m2' in df:\n",
    "        df.drop('surface_covered_in_m2', axis = 1, inplace = True)\n",
    "    if 'country_name' in df:\n",
    "        df.drop('country_name', axis = 1, inplace = True)\n",
    "    if 'price_aprox_local_currency' in df:\n",
    "        df.drop('price_aprox_local_currency', axis = 1, inplace = True)\n",
    "    if 'expenses' in df:\n",
    "        df.drop('expenses', axis = 1, inplace = True)\n",
    "    if 'properati_url' in df:\n",
    "        df.drop('properati_url', axis = 1, inplace = True)\n",
    "    if 'extra' in df:\n",
    "        df.drop('extra', axis = 1, inplace = True)\n",
    "    if 'geonames_id' in df:\n",
    "        df.drop('geonames_id', axis = 1, inplace = True)\n",
    "    if 'image_thumbnail' in df:\n",
    "        df.drop('image_thumbnail', axis = 1, inplace = True)\n",
    "    if 'operation' in df:\n",
    "        df.drop('operation', axis = 1, inplace = True)\n",
    "    if 'created_on' in df:\n",
    "        df.drop('created_on', axis = 1, inplace = True)\n",
    "    if 'lat-lon' in df:\n",
    "        df.drop('lat-lon', axis = 1, inplace = True)\n",
    "    if 'currency' in df:\n",
    "        df.drop('currency', axis = 1, inplace = True)\n",
    "    if 'title' in df:\n",
    "        df.drop('title', axis = 1, inplace = True)\n",
    "    if not isId and 'id' in df:\n",
    "        df.drop('id', axis = 1, inplace = True)\n",
    "    if 'price_aprox_local_currency' in df:\n",
    "        df.drop('price_aprox_local_currency', axis = 1, inplace = True)\n",
    "    if 'price_aprox_usd' in df and 'price' in df:\n",
    "        df.drop('price', axis = 1, inplace = True)\n",
    "    if 'extra' in df and 'price' in df:\n",
    "        df.drop('extra', axis = 1, inplace = True)\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# ADD DESCRIPTIONS COLUMNS\n",
    "#----------------------------------------------------------------------\n",
    "def addDescriptionColumns(df, columDict):\n",
    "    size = len(df.index)\n",
    "    description = list(df[columDict])\n",
    "    keys = description[0].keys()\n",
    "    for key in keys:\n",
    "        colum = []\n",
    "        for i in range(0, size):\n",
    "            value = description[i][key]\n",
    "            colum.append(value)\n",
    "        df[key] = colum\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# CHANGE PLACE WITH PARENT NAMES COLUMN TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def changePlaceWithParentsNamesColumn(df):    \n",
    "    listPlaces = df['place_with_parent_names'].tolist()\n",
    "    size = len(listPlaces)\n",
    "    for i in range(0, size):\n",
    "        listPlaces[i] = PlaceToNumber(listPlaces[i])\n",
    "    df['place_with_parent_names'] = listPlaces\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# PLACE TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def PlaceToNumber(x):\n",
    "    CF = 0\n",
    "    GBA = 1\n",
    "    x = str(x)\n",
    "    if 'Capital Federal' in x:\n",
    "        return CF\n",
    "    else:\n",
    "        return GBA\n",
    "#----------------------------------------------------------------------\n",
    "# CHANGE PROPERTY TYPE COLUMN TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def changePropertyTypeColumn(df):    \n",
    "    listPropertyType = df['property_type'].tolist()\n",
    "    size = len(listPropertyType)\n",
    "    for i in range(0, size):\n",
    "        listPropertyType[i] = propertyTypeToNumber(listPropertyType[i])\n",
    "    df['property_type'] = listPropertyType\n",
    "    return df\n",
    "#----------------------------------------------------------------------\n",
    "# PROPERTY TYPE TO NUMBER\n",
    "#----------------------------------------------------------------------\n",
    "def propertyTypeToNumber(x):\n",
    "    ph = 0\n",
    "    apartment = 1\n",
    "    house = 2\n",
    "    store = 3\n",
    "    if x.lower() == \"apartment\" or x.lower() == \"departamento\":\n",
    "        return apartment\n",
    "    if x.lower() == \"ph\":\n",
    "        return ph\n",
    "    if x.lower() == \"house\" or x.lower() == \"casa\":\n",
    "        return house\n",
    "    if x.lower() == \"store\":\n",
    "        return store\n",
    "#----------------------------------------------------------------------\n",
    "# Inclui cero si es casa y floor es nan\n",
    "#----------------------------------------------------------------------\n",
    "def aggFloor(floor):\n",
    "    if np.isnan(floor):\n",
    "        return 0\n",
    "    else:\n",
    "        return floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "archivesProceced = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop principal del filtrado de cada dataframe del set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properati-AR-2017-05-01-properties-sell.csv\n",
      "properati-AR-2014-01-01-properties-sell.csv\n",
      "properati-AR-2013-08-01-properties-sell.csv\n",
      "properati-AR-2015-04-01-properties-sell.csv\n",
      "properati-AR-2014-02-01-properties-sell.csv\n",
      "properati-AR-2013-09-01-properties-sell.csv\n",
      "properati-AR-2016-09-01-properties-sell.csv\n",
      "properati-AR-2017-01-01-properties-sell.csv\n",
      "properati-AR-2013-10-01-properties-sell.csv\n",
      "properati-AR-2015-05-01-properties-sell.csv\n",
      "properati-AR-2013-12-01-properties-sell.csv\n",
      "properati-AR-2014-05-01-properties-sell.csv\n",
      "properati-AR-2014-11-01-properties-sell.csv\n",
      "properati-AR-2014-09-01-properties-sell.csv\n",
      "properati-AR-2015-02-01-properties-sell.csv\n",
      "properati-AR-2016-08-01-properties-sell.csv\n",
      "properati-AR-2016-01-01-properties-sell.csv\n",
      "properati-AR-2016-11-01-properties-sell.csv\n",
      "properati-AR-2015-08-01-properties-sell.csv\n",
      "properati-AR-2017-03-01-properties-sell.csv\n",
      "properati-AR-2014-12-01-properties-sell.csv\n",
      "properati-AR-2015-12-01-properties-sell.csv\n",
      "properati-AR-2016-06-01-properties-sell.csv\n",
      "properati-AR-2015-06-01-properties-sell.csv\n",
      "properati-AR-2014-08-01-properties-sell.csv\n",
      "properati-AR-2016-04-01-properties-sell.csv\n",
      "properati-AR-2015-01-01-properties-sell.csv\n",
      "properati-AR-2014-06-01-properties-sell.csv\n",
      "properati-AR-2017-02-01-properties-sell.csv\n",
      "properati-AR-2013-11-01-properties-sell.csv\n",
      "properati-AR-2016-05-01-properties-sell.csv\n",
      "properati-AR-2015-03-01-properties-sell.csv\n",
      "properati-AR-2016-07-01-properties-sell.csv\n",
      "properati-AR-2014-07-01-properties-sell.csv\n",
      "properati-AR-2014-10-01-properties-sell.csv\n",
      "properati-AR-2015-11-01-properties-sell.csv\n",
      "properati-AR-2014-04-01-properties-sell.csv\n",
      "properati-AR-2017-06-06-properties-sell.csv\n",
      "properati-AR-2014-03-01-properties-sell.csv\n",
      "properati-AR-2017-06-01-properties-sell.csv\n",
      "properati-AR-2016-12-01-properties-sell.csv\n",
      "properati-AR-2015-10-01-properties-sell.csv\n",
      "properati-AR-2015-07-01-properties-sell.csv\n",
      "properati-AR-2015-09-01-properties-sell.csv\n",
      "properati-AR-2016-03-01-properties-sell.csv\n",
      "properati-AR-2017-04-01-properties-sell.csv\n",
      "properati-AR-2017-07-03-properties-sell.csv\n",
      "properati-AR-2016-02-01-properties-sell.csv\n",
      "properati-AR-2016-10-01-properties-sell.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Ruta de la carpeta con los archivos de datos modificados\n",
    "root = \"./properties/\"\n",
    "indexAcum = 0\n",
    "for archive in listdir(root):\n",
    "    if \".csv\" not in archive:\n",
    "        continue\n",
    "    if archive in archivesProceced:\n",
    "        continue\n",
    "    df = pd.read_csv(root + archive, low_memory = False)\n",
    "    \n",
    "    df = df.loc[df.place_with_parent_names.str.contains('Capital Federal') \\\n",
    "        | df.place_with_parent_names.str.contains('Bs.As. G.B.A.'), :]\n",
    "    \n",
    "    # Durante la carga de datos, se eliminan ciertas columnas que nos \n",
    "    # resultan irrelevantes para el trabajo.\n",
    "    df = filterUnnecesaryColumns(df, False)\n",
    "\n",
    "    # En algunos casos, es necesario renombrar algunas columnas\n",
    "    if 'price_aprox_usd' not in df:\n",
    "        df.rename(columns = {'price': 'price_aprox_usd'}, inplace = True)\n",
    "    if 'surface_total_in_m2' not in df:\n",
    "        df.rename(columns = {'surface_in_m2': 'surface_total_in_m2'}, \\\n",
    "            inplace = True)\n",
    "\n",
    "    # AquÃ­ reconvertimos algunas columnas a punto flotante\n",
    "    df.loc[:, 'price_aprox_usd'] = df.loc[:, ['price_aprox_usd']]\\\n",
    "        .apply(lambda x: float(x), axis = 1)\n",
    "    df.loc[:, 'price_usd_per_m2'] = df.loc[:, ['price_usd_per_m2']]\\\n",
    "        .apply(lambda x: float(x), axis = 1)\n",
    "\n",
    "    # Obtenemos el aÃ±o y mes del nombre de archivo\n",
    "    df = addDate(archive, df)\n",
    "\n",
    "    # Antes de filtrar me fijo si puedo recuperar la superficie del campo descripcion\n",
    "    df['surface_total_in_m2'] = getSurface(df)\n",
    "    \n",
    "    # AquÃ­ aplicamos el filtro antes declarado\n",
    "    df['filter1'] = df.loc[:, ['price_aprox_usd', 'price_usd_per_m2', \\\n",
    "            'surface_total_in_m2']].apply(lambda x: filterImposibles(x), axis = 1)\n",
    "    df = df[df['filter1'] == 1]\n",
    "    df.drop('filter1', axis = 1, inplace = True)\n",
    "    \n",
    "    size = len(df.index)\n",
    "    if size == 0:\n",
    "        continue\n",
    "    \n",
    "    df.loc[:, ['price_aprox_usd']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillPrice(x), axis = 1)\n",
    "    \n",
    "    df.loc[:, ['price_usd_per_m2']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillM2(x), axis = 1)\n",
    "    \n",
    "    df.loc[:, ['surface_total_in_m2']] = df.loc[:, ['price_aprox_usd', \\\n",
    "    'price_usd_per_m2', 'surface_total_in_m2']].apply(lambda x: fillSurface(x), axis = 1)\n",
    "    \n",
    "    df['filter2'] = df.loc[:, ['price_aprox_usd', 'price_usd_per_m2', \\\n",
    "    'surface_total_in_m2']].apply(lambda x: filterPercentage(x), axis = 1)\n",
    "    df = df[df['filter2'] == 1]\n",
    "    df.drop('filter2', axis = 1, inplace = True)\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    df.loc[:, ['place_with_parent_names']] = df.loc[:, ['place_with_parent_names']]\\\n",
    "    .apply(lambda x: PlaceToNumber(x), axis = 1)\n",
    "    df = changePropertyTypeColumn(df)\n",
    "    #df.loc[:, ['floor']] = df.loc[:, ['floor']].apply(lambda x: aggFloor(x), axis = 1)\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    # Si el filtrado es tal que me quedo sin dataframe, \n",
    "    # entonces salto a la siguiente iteracion\n",
    "    size = len(df.index)\n",
    "    if size == 0:\n",
    "        continue\n",
    "    \n",
    "    #Obtengo los campos de descripcion\n",
    "    df['rooms'] = getRooms(df)\n",
    "    df['description'] = crear_diccionario_descripcion(df)\n",
    "    df = addDescriptionColumns(df, 'description')\n",
    "    df.drop('description', axis = 1, inplace = True)\n",
    "    \n",
    "    # Finalmente, guardamos los archivos modificados.\n",
    "    indexAcum += size\n",
    "    newIndex = [i for i in range(indexAcum, indexAcum+size)]\n",
    "    df.reindex(newIndex)\n",
    "    print archive\n",
    "    properties.append(df)\n",
    "    archivesProceced.append(archive)\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "#Genero un nuevo csv con la concatenacion de todos ellos en uno solo\n",
    "general = pd.concat(properties)\n",
    "\n",
    "#Borro las columnas vacias\n",
    "for column in general.columns.values:\n",
    "    if 'unnamed' not in column.lower():\n",
    "        continue\n",
    "    general.drop(column, axis = 1, inplace = True)\n",
    "\n",
    "general.loc[:, ['date']] = pd.to_datetime(general['date'], errors = 'coerce')\n",
    "\n",
    "#Grabo la concatenacion en un unico csv\n",
    "try:\n",
    "    general.to_csv(\"propertiesConCat.csv\", index = True, header = True, \\\n",
    "        sep = ',', encoding = 'utf-8-sig')\n",
    "    print('Done')\n",
    "except value:\n",
    "    print('Error')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print len(archivesProceced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregado de datos de extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# CHANGE STRING COMMA FOR POINT\n",
    "#---------------------------------------------------------\n",
    "def changeStringCommaForPoint(string):\n",
    "    point = \".\"\n",
    "    split = string.split(',')\n",
    "    return point.join(split)\n",
    "#---------------------------------------------------------\n",
    "# CHANGE STRING LIST COMMA FOR POINT\n",
    "#---------------------------------------------------------\n",
    "def changeStringListCommaForPoint(stringList):\n",
    "    aux = []\n",
    "    for string in stringList:\n",
    "        aux.append(changeStringCommaForPoint(string))\n",
    "    return aux\n",
    "#---------------------------------------------------------\n",
    "# MANHATTAN DISTANCE\n",
    "#---------------------------------------------------------\n",
    "def ManhattanDistance(lat1, lon1, lat2, lon2):\n",
    "    # pasamos la diferencia a metros (90Â° son 10000 Km)\n",
    "    dlat = abs(lat1-lat2) * (10000/90)\n",
    "    # pasamos la diferencia a metros (360Â° son 40000 Km)\n",
    "    dlon = abs(lon1-lon2) * (40000/360) \n",
    "    distKM = ( (dlat ** 2) + (dlon ** 2) ) ** (0.5)\n",
    "    return float(distKM * 1000)\n",
    "#---------------------------------------------------------\n",
    "# DISTANCE ANALYSIS\n",
    "#---------------------------------------------------------\n",
    "def distanceAnalysis(df, extraDf, lat, lon, distanceName):\n",
    "    # x) lon\n",
    "    # y) lat\n",
    "    extraDf.loc[:, [lon]] = extraDf.loc[:, [lon]].apply(lambda x: float(x), axis = 1)\n",
    "    extraDf.loc[:, [lat]] = extraDf.loc[:, [lat]].apply(lambda x: float(x), axis = 1)\n",
    "    extraDf = extraDf[~np.isnan(extraDf[lon]) | ~np.isnan(extraDf[lat])]\n",
    "\n",
    "    df = df[~np.isnan(df['lon']) | ~np.isnan(df['lat'])]\n",
    "\n",
    "    latDf = df['lat'].tolist()\n",
    "    lonDf = df['lon'].tolist()\n",
    "    x = extraDf[lon].tolist()\n",
    "    y = extraDf[lat].tolist()\n",
    "\n",
    "    distances = []\n",
    "    minor = 0\n",
    "\n",
    "    for i in range(0, len(latDf)):\n",
    "        minor = ManhattanDistance(y[0], x[0], latDf[i], lonDf[i])\n",
    "        for j in range(1, len(x)):\n",
    "            dist = ManhattanDistance(y[j], x[j], latDf[i], lonDf[i])\n",
    "            if (dist < minor):\n",
    "                minor = dist\n",
    "        distances.append(minor)\n",
    "\n",
    "    df[distanceName] = distances\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG SUBWAYS\n",
    "#---------------------------------------------------------\n",
    "def aggSubways(df):\n",
    "    subways = pd.read_csv(\"./extra/estaciones-de-subte.csv\", low_memory = False)\n",
    "    df = distanceAnalysis(df, subways, 'Y', 'X', 'distanceSubway')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG UNIVERSITIES\n",
    "#---------------------------------------------------------\n",
    "def aggUniversities(df):\n",
    "    universities = pd.read_csv(\"./extra/universidades.csv\", low_memory = False, sep=';')\n",
    "    df = distanceAnalysis(df, universities, 'LAT', 'LNG', 'distanceUniversities')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG HOSPITALES\n",
    "#---------------------------------------------------------\n",
    "def aggHospitales(df):\n",
    "    hospitales = pd.read_csv(\"./extra/hospitales.csv\", low_memory = False, sep=';')\n",
    "    df = distanceAnalysis(df, hospitales, 'LAT', 'LNG', 'distanceHospitales')\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# AGG BUS STOPS\n",
    "#---------------------------------------------------------\n",
    "def aggBusStops(df):\n",
    "    busStops = pd.read_csv(\"./extra/paradas-de-colectivo.csv\", low_memory = False, sep=';')\n",
    "    busStops['X'] = changeStringListCommaForPoint(busStops['X'].tolist())\n",
    "    busStops['Y'] = changeStringListCommaForPoint(busStops['Y'].tolist())\n",
    "    df = distanceAnalysis(df, busStops, 'Y', 'X', 'distanceBusStops')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de filtrado y acomodamiento de los datos para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# FUNCTIONS OF PROCCES OF TRAIN DATA\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# HASH PLACES\n",
    "#---------------------------------------------------------\n",
    "def hashPlaces(df, predictDf):\n",
    "    placesHash = {}\n",
    "    groupedPlaces = df.loc[:, ['place_name', 'floor']]\\\n",
    "    .groupby('place_name').agg([np.size]).reset_index()\n",
    "    places = groupedPlaces['place_name'].tolist()\n",
    "    groupedPlaces = predictDf.loc[:, ['place_name', 'floor']]\\\n",
    "    .groupby('place_name').agg([np.size]).reset_index()\n",
    "    places = places + groupedPlaces['place_name'].tolist()\n",
    "    size = len(places)\n",
    "    for i in range(0, size):\n",
    "        if places[i] in placesHash:\n",
    "            continue\n",
    "        placesHash[places[i]] = 0\n",
    "    keys = placesHash.keys()\n",
    "    size = len(keys)\n",
    "    for i in range(0, size):\n",
    "        placesHash[keys[i]] = i\n",
    "    placesHash[\"no place\"] = size\n",
    "    return placesHash\n",
    "#---------------------------------------------------------\n",
    "# PROCCES NO PLACES\n",
    "#---------------------------------------------------------\n",
    "def processNoPlaces(df):\n",
    "    places = df['place_name'].tolist()\n",
    "    size = len(places)\n",
    "    for i in range(0, size):\n",
    "        if type(places[i]) != type(\"\"):\n",
    "            places[i] = \"no place\"\n",
    "    df['place_name'] = places\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# CNVERT PLACES TO HASH NUMBER\n",
    "#---------------------------------------------------------\n",
    "def convertPlacesToHashNumber(df, placesHash):   \n",
    "    placesDataTrain = df['place_name'].tolist()\n",
    "    size = len(placesDataTrain)\n",
    "    count = 0\n",
    "    hashSize = len(placesHash.keys())\n",
    "    for i in range(0, size):\n",
    "        placeName = placesDataTrain[i]\n",
    "        placesDataTrain[i] = placesHash[placeName]\n",
    "    df['place_name'] = placesDataTrain\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# DELETE EXTRA COLUMNS\n",
    "#---------------------------------------------------------\n",
    "def deleteExtraColumns(df):\n",
    "    extra = [\n",
    "        'sector de juegos infantiles',\n",
    "        'seguridad las 24 hs',\n",
    "        'cancha de tenis',\n",
    "        'club house',\n",
    "        'cochera',\n",
    "        'comedor',\n",
    "        'futbol 5',\n",
    "        'living',\n",
    "        'pileta',\n",
    "        'piscina',\n",
    "        'rooms'\n",
    "    ]\n",
    "    columns = [\n",
    "        'price_aprox_usd.1', \n",
    "        'Unnamed: 0', \n",
    "        'price_per_m2', \n",
    "        'date',\n",
    "        'floor', \n",
    "        'state_name', \n",
    "        'price_usd_per_m2'\n",
    "    ]\n",
    "    df.drop(columns+extra, axis = 1, inplace = True)\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if \"unnamed\" in column.lower():\n",
    "            df.drop(column, axis = 1, inplace = True) \n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# DELETE TRASH COLUMNS\n",
    "#---------------------------------------------------------\n",
    "def deleteTrashColumns(df):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if \"unnamed\" in column.lower():\n",
    "            df.drop(column, axis = 1, inplace = True) \n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# PROCESS TRAIN SET\n",
    "#---------------------------------------------------------\n",
    "def ProcessTrainSet(df, withPrice):\n",
    "    df.dropna(axis=0, how='any', subset=list(df.columns.values), inplace=True)\n",
    "    \n",
    "    target = df['price_aprox_usd'].tolist()\n",
    "    if withPrice:\n",
    "        df.drop('price_aprox_usd', axis = 1, inplace = True)\n",
    "    \n",
    "    columns = list(df.columns.values)\n",
    "    data = list(df.values)\n",
    "    return data, target\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# FUNCTIONS OF PROCCES OF TEST DATA\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# DELETE COLUMNS TEST DATA\n",
    "#---------------------------------------------------------\n",
    "def deleteColumnsTestData(df, columnsToEvaluate):\n",
    "    columns = list(df.columns.values)\n",
    "    for column in columns:\n",
    "        if column in columnsToEvaluate:\n",
    "            continue\n",
    "        df.drop(column, axis = 1, inplace = True)\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# PRE PROCCES TEST DATA\n",
    "#---------------------------------------------------------\n",
    "def preProcessTestData(df, placesHash):\n",
    "    df = filterUnnecesaryColumns(df, True)\n",
    "    df['rooms'] = getRooms(df)\n",
    "    #df['surface_total_in_m2'] = getSurface(df)\n",
    "    df['description'] = crear_diccionario_descripcion(df)\n",
    "    df = processNoPlaces(df)\n",
    "    df = convertPlacesToHashNumber(df, placesHash)\n",
    "    df = addDescriptionColumns(df, 'description')\n",
    "    df = changePlaceWithParentsNamesColumn(df)\n",
    "    df = changePropertyTypeColumn(df)\n",
    "    return df\n",
    "#---------------------------------------------------------\n",
    "# GET DATA AS LIST\n",
    "#---------------------------------------------------------\n",
    "def GetDataAsList(df):\n",
    "    columns = list(df.columns.values)\n",
    "    data = list(df.values)\n",
    "    return data\n",
    "#------------------------------------------------------\n",
    "# GET AVG OF LIST\n",
    "#------------------------------------------------------\n",
    "def getAvgOfList(x):\n",
    "    surfaces = x['surface_total_in_m2'].tolist()\n",
    "    surfaces = [x for x in surfaces if (not np.isnan(x) and x > 0)]\n",
    "    size = len(surfaces)\n",
    "    if size == 0:\n",
    "        return np.nan\n",
    "    surfaces.sort()\n",
    "    return surfaces[size/2]\n",
    "\n",
    "#------------------------------------------------------\n",
    "# CREATE DICC\n",
    "#------------------------------------------------------\n",
    "def createDicc(keys, values):\n",
    "    dicc = {}\n",
    "    size = len(keys)\n",
    "    for i in range(0, size):\n",
    "        dicc[keys[i]] = values[i]\n",
    "    return dicc\n",
    "#------------------------------------------------------\n",
    "# GET AVG SURFACE BY PLACE\n",
    "#------------------------------------------------------\n",
    "def getAvgSurfaceByPlace(df):\n",
    "    aux = df.loc[:, ['surface_total_in_m2', 'place_name']]\\\n",
    "    .groupby('place_name').apply(lambda x: getAvgOfList(x)).reset_index()\n",
    "    \n",
    "    placesName = aux['place_name'].tolist()\n",
    "    surfaces = aux[0].tolist()\n",
    "    dicc = createDicc(placesName, surfaces)\n",
    "    \n",
    "    placesNameDF = df['place_name'].tolist()\n",
    "    surfacesDF = df['surface_total_in_m2'].tolist()\n",
    "    size = len(surfacesDF)\n",
    "    for i in range(0, size):\n",
    "        if not np.isnan(surfacesDF[i]) and surfacesDF[i] > 0:\n",
    "            continue\n",
    "        surfacesDF[i] = dicc[placesNameDF[i]]\n",
    "    df['surface_total_in_m2'] = surfacesDF\n",
    "    return df\n",
    "\n",
    "#------------------------------------------------------\n",
    "# GET AVG SURFACE BY PLACE\n",
    "#------------------------------------------------------\n",
    "def getAvgSurfaceByState(df):\n",
    "    aux = df.loc[:, ['surface_total_in_m2', 'state_name']]\\\n",
    "    .groupby('state_name').apply(lambda x: getAvgOfList(x)).reset_index()\n",
    "    \n",
    "    states = aux['state_name'].tolist()\n",
    "    surfaces = aux[0].tolist()\n",
    "    dicc = createDicc(states, surfaces)\n",
    "    \n",
    "    statesDF = df['state_name'].tolist()\n",
    "    surfacesDF = df['surface_total_in_m2'].tolist()\n",
    "    size = len(surfacesDF)\n",
    "    for i in range(0, size):\n",
    "        if not np.isnan(surfacesDF[i]) and surfacesDF[i] > 0:\n",
    "            continue\n",
    "        ## Caso especial donde Buenos Aire interior \n",
    "        ## hay solo una propiedad con todo en nan\n",
    "        if statesDF[i] == 'Buenos Aires Interior':\n",
    "            avgGBAO = dicc['Bs.As. G.B.A. Zona Oeste']\n",
    "            avgGBAS = dicc['Bs.As. G.B.A. Zona Sur']\n",
    "            avgBAI = (avgGBAO + avgGBAS) / 2\n",
    "            surfacesDF[i] = avgBAI\n",
    "        else:\n",
    "            surfacesDF[i] = dicc[statesDF[i]]\n",
    "    df['surface_total_in_m2'] = surfacesDF\n",
    "    return df\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# SAVE FINAL DF\n",
    "#---------------------------------------------------------\n",
    "def saveFinalDF(predictions, ids):\n",
    "    aData = {'id': ids, 'price_usd': predictions}\n",
    "    final = pd.DataFrame(data = aData)\n",
    "    \"\"\"final['id'] = ids\n",
    "    final['price_usd'] = predictions\n",
    "    final = final.reset_index()\n",
    "    final.drop('index', axis = 1, inplace = True)\"\"\"\n",
    "    final.to_csv(\"properati_dataset_sample_submision.csv\", \\\n",
    "    index = True, header = True, sep = ',', encoding = 'utf-8-sig')\n",
    "    return final\n",
    "#---------------------------------------------------------\n",
    "# FILL COORDS BY STATE NAME\n",
    "#---------------------------------------------------------\n",
    "def fillCoordsByStateName(df, coordinatesDicc):\n",
    "    ok = True\n",
    "    for key in coordinatesDicc.keys():\n",
    "        coord = coordinatesDicc[key]\n",
    "        if not np.isnan(coord[0]) and not np.isnan(coord[0]):\n",
    "            continue\n",
    "        ok = False\n",
    "        break\n",
    "    if ok:\n",
    "        return\n",
    "        \n",
    "    aux = df.loc[:, ['state_name', 'lat', 'lon']]\\\n",
    "    .groupby('state_name').agg([np.mean, np.size]).reset_index()\n",
    "    aux = aux[aux[('lat', 'size')] >= 20]\n",
    "    \n",
    "    states = aux['state_name'].tolist()\n",
    "    latList = aux[('lat', 'mean')].tolist()\n",
    "    lonList = aux[('lon', 'mean')].tolist()\n",
    "    size = len(states)\n",
    "\n",
    "    latGBA = 0\n",
    "    lonGBA = 0\n",
    "    latCF = 0\n",
    "    lonCF = 0\n",
    "    for i in range(0, size):\n",
    "        if states[i] == \"Bs.As. G.B.A. Zona Oeste\" or states[i] == \"Bs.As. G.B.A. Zona Sur\":\n",
    "            latGBA += latList[i]\n",
    "            lonGBA += lonList[i]\n",
    "        else:\n",
    "            latCF += latList[i]\n",
    "            lonCF += lonList[i]\n",
    "    latGBA = latGBA/2\n",
    "    lonGBA = lonGBA/2\n",
    "    \n",
    "    states = df['state_name'].tolist()\n",
    "    places = df['place_name'].tolist()\n",
    "    latList = df['lat'].tolist()\n",
    "    lonList = df['lon'].tolist()\n",
    "    size = len(states)\n",
    "    for i in range(0, size):\n",
    "        coord = coordinatesDicc[places[i]]\n",
    "        if not np.isnan(coord[0]) and not np.isnan(coord[1]):\n",
    "            continue\n",
    "        if states[i] == 'Capital Federal':\n",
    "            coordinatesDicc[places[i]] = [latCF, lonCF]\n",
    "        else:\n",
    "            coordinatesDicc[places[i]] = [latGBA, lonGBA]\n",
    "    return coordinatesDicc\n",
    "#---------------------------------------------------------\n",
    "# CREATE DICT OF COORDS BY PLACES\n",
    "#---------------------------------------------------------\n",
    "def createDictOfCoordsByPlaces(df):\n",
    "    aux = df.loc[:, ['place_name', 'lat', 'lon']]\\\n",
    "    .groupby('place_name').agg([np.mean, np.size]).reset_index()\n",
    "    coordinatesDicc = {}\n",
    "    places = aux['place_name'].tolist()\n",
    "    latList = aux[('lat', 'mean')].tolist()\n",
    "    lonList = aux[('lon', 'mean')].tolist()\n",
    "    size = len(places)\n",
    "    for i in range(0, size):\n",
    "        coordinatesDicc[places[i]] = [latList[i], lonList[i]]\n",
    "    coordinatesDicc = fillCoordsByStateName(df, coordinatesDicc)\n",
    "    return coordinatesDicc\n",
    "#---------------------------------------------------------\n",
    "# FILL COORDINATES DATA TEST\n",
    "#---------------------------------------------------------\n",
    "def fillCoordinatesDataTest(df):\n",
    "    coordinatesDicc = createDictOfCoordsByPlaces(df)\n",
    "    \n",
    "    places = df['place_name'].tolist()\n",
    "    latList = df['lat'].tolist()\n",
    "    lonList = df['lon'].tolist()\n",
    "    size = len(places)\n",
    "    \n",
    "    for i in range(0, size):\n",
    "        if np.isnan(latList[i]):\n",
    "            latList[i] = coordinatesDicc[places[i]][0]\n",
    "        if np.isnan(lonList[i]):\n",
    "            lonList[i] = coordinatesDicc[places[i]][1] \n",
    "    df['lat'] = latList\n",
    "    df['lon'] = lonList\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"propertiesConCat.csv\", low_memory = False)\n",
    "predictDf = pd.read_csv(\"properati_dataset_testing_noprice.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/.local/lib/python2.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"df.loc[:,'date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.loc[df['date'].dt.year >= 2016]\n",
    "df = getAvgSurfaceByPlace(df)\n",
    "df = getAvgSurfaceByState(df)\n",
    "df = aggSubways(df)\n",
    "df = aggUniversities(df)\n",
    "df = aggHospitales(df)\n",
    "#df = aggBusStops(df)\n",
    "\n",
    "placesHash = hashPlaces(df, predictDf)\n",
    "df = processNoPlaces(df)\n",
    "df = convertPlacesToHashNumber(df, placesHash)\n",
    "df = deleteExtraColumns(df)\n",
    "df.to_csv(\"dataTrain.csv\", index = True, header = True, sep = ',', \n",
    "          encoding = 'utf-8-sig')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataTrain.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deleteTrashColumns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain, targetTrain = ProcessTrainSet(df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:  269446\n",
      "target size:  269446\n",
      "columns size:  9\n"
     ]
    }
   ],
   "source": [
    "columns = list(df.columns.values)\n",
    "print \"data size: \", len(dataTrain)\n",
    "print \"target size: \", len(targetTrain)\n",
    "print \"columns size: \", len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproceso del set de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lat',\n",
       " 'lon',\n",
       " 'place_name',\n",
       " 'place_with_parent_names',\n",
       " 'property_type',\n",
       " 'surface_total_in_m2',\n",
       " 'distanceSubway',\n",
       " 'distanceUniversities',\n",
       " 'distanceHospitales']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsToEvaluate = columns\n",
    "columnsToEvaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictDf = pd.read_csv(\"properati_dataset_testing_noprice.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"predictDf = getAvgSurfaceByPlace(predictDf)\n",
    "predictDf = getAvgSurfaceByState(predictDf)\n",
    "predictDf = fillCoordinatesDataTest(predictDf)\n",
    "predictDf = fillCoordinatesDataTest(predictDf)\n",
    "predictDf = aggSubways(predictDf)\n",
    "predictDf = aggUniversities(predictDf)\n",
    "predictDf = aggHospitales(predictDf)\n",
    "aux = predictDf\n",
    "places = aux['place_name'].tolist()\n",
    "predictDf = preProcessTestData(predictDf, placesHash)\n",
    "ids = predictDf['id']\n",
    "predictDf = deleteColumnsTestData(predictDf, columnsToEvaluate)\n",
    "data = GetDataAsList(predictDf)\n",
    "predictDf.to_csv(\"dataTest.csv\", index = True, header = True, sep = ',', \n",
    "          encoding = 'utf-8-sig')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:  14166\n",
      "df size:  14166\n",
      "columns size:  9\n"
     ]
    }
   ],
   "source": [
    "print \"data size: \", len(data)\n",
    "print \"df size: \", len(predictDf.index)\n",
    "print \"columns size: \", len(predictDf.columns.values)\n",
    "#row should be 14166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictDf = pd.read_csv(\"dataTest.csv\", low_memory = False)\n",
    "predictDf = deleteTrashColumns(predictDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcces sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(dataTrain, targetTrain, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entreno el set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision sample:  0.098377413181\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(normalize = True)\n",
    "lr.fit(dataTrain, targetTrain)\n",
    "lr.score(dataTrain, targetTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = saveFinalDF(predictions, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3632</td>\n",
       "      <td>1.247484e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3633</td>\n",
       "      <td>5.288541e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2263404</td>\n",
       "      <td>8.702557e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2263405</td>\n",
       "      <td>4.927972e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2263406</td>\n",
       "      <td>6.199079e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     price_usd\n",
       "0     3632  1.247484e+07\n",
       "1     3633  5.288541e+06\n",
       "2  2263404  8.702557e+06\n",
       "3  2263405  4.927972e+06\n",
       "4  2263406  6.199079e+06"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformacion no lineal a lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (Este metodo da precios demasiado altos y es peor que el lineal hecho en el paso anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tomamos como ejemplo una funciÃ³n f que toma la forma :  f(x) = a + bx + cxÂ²\n",
    "\n",
    "La funciÃ³n f es no lineal en funciÃ³n de x pero si es lineal en funciÃ³n de los parÃ¡metros desconocidos a, b, y c. O visto de otra manera: podemos sustituir nuestras variables x por un array z tal que: z = [1, x, xÂ²]. Con el que podrÃ­amos reescribir nuestra funciÃ³n f como f(z) = a z0 + bz1 + c*z2\n",
    "\n",
    "Scikit-learn tiene un objeto PolynomialFeatures que nos va a servir para convertir nuestra variable x en un array z del tipo z = [1, x, x2, â¦, n^n], que es lo que nos interesa.\n",
    "\n",
    "El resultado de esa transformaciÃ³n se la pasamos a nuestro modelo Ridge. Para facilitar la tarea en este tipo de casos âdonde se realizan varios pasos que van desde el pre-tratamiento de los datos hasta un posible post-tratamiento pasando por el entrenamientoâ, podemos hacer uso de las Pipeline que nos permiten encadenar multiples estimadores en uno. Esto es especialmente Ãºtil cuando hay secuencia de pasos predefinidos en el procesado de datos con, por ejemplo, selecciÃ³n de atributos, normalizaciÃ³n y clasificaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18679831133031699"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "z = poly.fit_transform(xTrain)\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(z, yTrain)\n",
    "lr.score(z, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " precision sample:  0.152641099085\n"
     ]
    }
   ],
   "source": [
    "zTest = poly.fit_transform(xTest)\n",
    "predictionsSamplePlynomial = lr.predict(zTest)\n",
    "print \"precision sample: \", lr.score(zTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTransformed = poly.fit_transform(data)\n",
    "predictions = lr.predict(DataTransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = saveFinalDF(predictions, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3632</td>\n",
       "      <td>1.001339e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3633</td>\n",
       "      <td>8.158107e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2263404</td>\n",
       "      <td>5.498861e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2263405</td>\n",
       "      <td>8.590666e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2263406</td>\n",
       "      <td>8.654199e+20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     price_usd\n",
       "0     3632  1.001339e+21\n",
       "1     3633  8.158107e+21\n",
       "2  2263404  5.498861e+21\n",
       "3  2263405  8.590666e+20\n",
       "4  2263406  8.654199e+20"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': [i for i in range(50, 100)]}\n",
    "gs = GridSearchCV(DecisionTreeRegressor(), params, cv=3, n_jobs=1, verbose=1, scoring=None, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision test:  0.816363469132\n"
     ]
    }
   ],
   "source": [
    "gs.fit(xTrain, yTrain)\n",
    "yTestPrediction = gs.predict(xTest)\n",
    "print \"precision test: \", gs.score(xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "gs.fit(dataTrain, targetTrain)\n",
    "predictionDT = gs.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = saveFinalDF(predictionDT, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3632</td>\n",
       "      <td>690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3633</td>\n",
       "      <td>155000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2263404</td>\n",
       "      <td>71000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2263405</td>\n",
       "      <td>177000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2263406</td>\n",
       "      <td>55000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  price_usd\n",
       "0     3632   690000.0\n",
       "1     3633   155000.0\n",
       "2  2263404    71000.0\n",
       "3  2263405   177000.0\n",
       "4  2263406    55000.0"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [4,6],\n",
    "    'min_samples_leaf': [3, 5, 9],\n",
    "    'max_features': [1.0, 0.3, 0.1]\n",
    "}\n",
    "gs = GridSearchCV(RandomForestRegressor(), params, cv=3, n_jobs=1, verbose=1, scoring=None, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57395798407226906"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(xTrain, yTrain)\n",
    "gs.score(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score test:  0.502154656875\n"
     ]
    }
   ],
   "source": [
    "gs.predict(xTest)\n",
    "print \"score test: \", gs.score(xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score full dataTrain:  0.552024792787\n"
     ]
    }
   ],
   "source": [
    "gs.fit(dataTrain, targetTrain)\n",
    "print \"score full dataTrain: \", gs.score(xTrain, yTrain)\n",
    "RFPrediction = gs.predict(data)\n",
    "final = saveFinalDF(RFPrediction, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3632</td>\n",
       "      <td>780727.577026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3633</td>\n",
       "      <td>311615.522128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2263404</td>\n",
       "      <td>344923.313484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2263405</td>\n",
       "      <td>113885.326676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2263406</td>\n",
       "      <td>113885.326676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      price_usd\n",
       "0     3632  780727.577026\n",
       "1     3633  311615.522128\n",
       "2  2263404  344923.313484\n",
       "3  2263405  113885.326676\n",
       "4  2263406  113885.326676"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
